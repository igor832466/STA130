{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c65c450",
   "metadata": {},
   "source": [
    ">Igor Martins Frota\n",
    ">\n",
    ">1011330490\n",
    ">\n",
    ">14-11-2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88cdfe3",
   "metadata": {},
   "source": [
    "# Pre-lecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9fbac5",
   "metadata": {},
   "source": [
    "## 1.\n",
    "\n",
    "**1.** Simple Linear Regression vs Multiple Linear Regression\n",
    "\n",
    ">**Simple Linear Regression**\n",
    ">\n",
    ">>$ Y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i$\n",
    ">\n",
    ">>Uses a single predictor variable ($x_i$) to predict a continuous outcome variable ($Y_i$), assuming a linear relationship between both variables modelled by a linear function.\n",
    ">\n",
    ">**Multiple Linear Regression**\n",
    ">\n",
    ">>$ Y_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + ... + \\beta_n x_{ni} + \\epsilon_i$\n",
    ">>\n",
    ">>Uses multiple predictor varaibles ($x_{ni}$) to predict an outcome variable ($Y_i$). This models assumes there is a more complex relationship between the predictor and outcome variables that cannot be modelled by a simple linear regression model and involves multiple factors.\n",
    ">\n",
    ">**Difference and Benefits of Multiple Linear Regression**\n",
    ">>\n",
    ">>The difference between these two regression models lie in the number of factors each takes into account and the complexity of the relationship between the predictor and outcome variable. The simple linear regression model assumes there is only one contributing factor ($\\beta_1$) in the relationship between the variables and that this can be modelled linearly. However, the multiple linear regression model assumes multiple contributing factors ($\\beta_n$) and it can model a complex relationship. Additionally, this model also provides insight into the individual impact of each factor by using $x_{ni}$, which can essentially turn variables \"on\" or \"off\". \n",
    "\n",
    "**2. Continuous vs Indicator Variable**\n",
    "\n",
    ">A continuous variable is one that can take on any possible numeric values in a given range. Its linear form is represented by the equation \n",
    "$$Y_i = \\beta_0 + \\beta_1x_i + \\epsilon_i$$\n",
    ">An indicator variable, on the other hand, is a binary variable that represents the existance of a categorical characteristic. It is represented by the equation \n",
    "$$Y_i = \\beta_0 + \\beta_1 * 1_{[\\text{group}]} (k_i) + \\epsilon_i$$\n",
    "As a binary variable, it either exists (when $k_i$ is multiplied by $1$) or does not (when $k_i$ is multiplied by $0$).\n",
    ">\n",
    "> The difference between both variables is in how they represent and represent the relationship between data. In the contex of simple linear regression, a continuous predictor variable implies a linear relationship between the predictor and outcome varaiables. However, an indicator variable focuses on the mean difference in both variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c425a955",
   "metadata": {},
   "source": [
    "### NotebookLM Session Summary\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>View</u></summary>\n",
    "\n",
    ">Okay, here is a summary of everything you have discussed:\n",
    ">\n",
    ">### Simple and Multiple Linear Regression\n",
    ">\n",
    ">You have discussed the differences between simple and multiple linear regression, as well as continuous and indicator variables.\n",
    ">\n",
    ">*   **Simple linear regression** uses one predictor variable to model a continuous outcome variable. \n",
    ">*   **Multiple linear regression** uses multiple predictor variables to model a continuous outcome. Multiple linear regression is more beneficial because it can: \n",
    ">    *   model more complex relationships; \n",
    ">    *   show how much each predictor contributes to the outcome; and \n",
    ">    *   achieve higher predictive accuracy. \n",
    ">\n",
    ">### Continuous and Indicator Variables\n",
    ">\n",
    ">**Continuous Variables**\n",
    ">\n",
    ">*   Continuous variables are a type of numerical variable that can take on an infinite number of values. \n",
    ">*   Examples of continuous variables include:\n",
    ">    *   height\n",
    ">    *   weight\n",
    ">    *   temperature\n",
    ">    *   time. \n",
    ">\n",
    ">**Indicator Variables**\n",
    ">\n",
    ">*   Indicator variables are also called **dummy variables** and they represent categorical data in a regression analysis. \n",
    ">*   Indicator variables take on a value of 1 if a category is present and 0 if it is not.\n",
    ">*   You need to use indicator variables in regression models because regression models use numerical data. \n",
    ">\n",
    ">### Simple Linear Regression Equations\n",
    ">\n",
    ">**Simple linear regression equation with a continuous variable:**\n",
    ">\n",
    ">$Y_i = \\beta_0 + \\beta_1x_i + \\epsilon_i$ where $\\epsilon_i \\sim \\mathcal{N}(0, \\sigma)$\n",
    ">\n",
    ">**Simple linear regression equation with an indicator variable:**\n",
    ">\n",
    ">Not specified in sources or conversation history.\n",
    ">\n",
    ">### Multiple Linear Regression Equations\n",
    ">\n",
    ">**Multiple linear regression equation with continuous variables:**\n",
    ">\n",
    ">$Y_i = \\beta_0 + \\beta_1x_{1i} + \\beta_2x_{2i} + \\epsilon_i$ where $\\epsilon_i \\sim \\mathcal{N}(0, \\sigma)$\n",
    ">\n",
    ">**Multiple linear regression equation with indicator variables:**\n",
    ">\n",
    ">$Y_i = \\beta_0 + \\beta_1x_{1i} + \\beta_2x_{2i} + \\epsilon_i$ where $\\epsilon_i \\sim \\mathcal{N}(0, \\sigma)$ \n",
    ">\n",
    ">**Multiple linear regression equation with an interaction between continuous and indicator variables:**\n",
    ">\n",
    ">$Y_i = \\beta_0 + \\beta_1x_{1i} + \\beta_2x_{2i} + \\beta_3(x_{1i}x_{2i}) + \\epsilon_i$ where $\\epsilon_i \\sim \\mathcal{N}(0, \\sigma)$ \n",
    ">\n",
    ">### Using Indicator Variables\n",
    ">\n",
    ">When you have a categorical variable with more than 2 levels, you can use multiple indicator variables to represent the original variable. For example, if you have 3 categories (A, B, and C) you would use 2 indicator variables - one for 'B' and one for 'C' and A would be the **baseline group**. This means that each indicator variable will show the difference in the outcome between its corresponding group and the baseline group.  You discussed this example in the context of the advertising budget. In your example, a high advertising budget was coded with a 1 and a low advertising budget was coded with a 0. \n",
    ">\n",
    ">### Interactions\n",
    ">\n",
    ">*   Interactions are used to model situations where the relationship between one predictor variable and the outcome changes based on the value of a different predictor variable. \n",
    ">*   Interactions can be between continuous or indicator variables, or both. \n",
    ">\n",
    ">### Model Building\n",
    ">\n",
    ">You used the example of TV advertising and online advertising to look at different models and how well they **generalize** to the data. You looked at:\n",
    ">\n",
    ">*   **model3\\_fit:** Not explained in sources or conversation history\n",
    ">*   **model4\\_fit:** Creates a 'design matrix' using predictor variables to predict the outcome. Multicollinearity in the design matrix causes a lack of generalization of the predictions from the model. \n",
    ">*   **model5\\_linear\\_form:** Builds on model3\\_fit and model4\\_fit. \n",
    ">*   **model6\\_linear\\_form:** Builds on model5\\_linear\\_form. \n",
    ">*   **model7\\_linear\\_form:** Builds on model6\\_linear\\_form. While model 7 seems to generalize well and has good performance, it is more complex. More complex models have a higher risk of overfitting to the training data and not performing well in the testing data. \n",
    ">\n",
    ">The sources also discuss the tools used in STA130  and provide information about assignments.  \n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d677ec78",
   "metadata": {},
   "source": [
    "## 4.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e8f94e",
   "metadata": {},
   "source": [
    "A model can have both a low R-squared value and \"good\" p-values that the variables' coefficient are larger than 10 and strong evidence against a null hypothesis of no effect (i.e. $H_0$: $\\beta_1$ = 0). This is because of many reasons, such as other factors that are not reflected in the model. This contributes to p-values providing evidence against a null hypothesis, since the predictor variables do predict the outcome variables with coefficients larger than 10. However, as some factors are not taken into account, the R-sqaured value is still very low (17.6%)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee257201",
   "metadata": {},
   "source": [
    "### NotebookLM Session Summary\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>View</u></summary>\n",
    "\n",
    ">### This Session's Focus: R-Squared and p-values\n",
    ">\n",
    ">This session focused on understanding the relationship between R-squared and p-values in a multilinear regression model, particularly when you have a small R-squared value with good p-values.\n",
    ">\n",
    ">*   **R-squared** ($R^2$) is the \"coefficient of determination\" and it measures the proportion of variation in the outcome variable explained by the predictor variables in a regression model. For example, an R-squared of 0.75 means that 75% of the variance in the outcome variable is explained by the model. The remaining 25% is unexplained.\n",
    ">\n",
    ">*   **p-values** assess the statistical significance of the relationships between individual predictors and the outcome variable. A p-value of less than 0.05 is generally considered statistically significant, and a p-value greater than 0.1 indicates no evidence against the null hypothesis.\n",
    ">\n",
    ">### What does a small R-squared with good p-values mean?\n",
    ">\n",
    ">A small R-squared with good p-values means that while the predictors are statistically significant, the model is not able to explain much of the variation in the outcome variable. There are several reasons why this might happen:\n",
    ">\n",
    ">*   **Omitted Variables:** The model might be missing important predictors that contribute to the outcome variable's variation.\n",
    ">*   **Multicollinearity:** Multicollinearity is when two or more predictors are highly correlated. When predictors are highly correlated, it can be hard to determine the individual effects of each predictor on the outcome variable, and the model may not generalize well to new data.\n",
    ">*   **Complex Relationships:** The relationship between the predictors and the outcome variable might be more complex than a linear model can capture.\n",
    ">\n",
    ">### What to consider when you have a small R-squared with good p-values\n",
    ">\n",
    ">When you encounter a small R-squared with good p-values, you should consider:\n",
    ">\n",
    ">*   **The purpose of the model:** If the goal is prediction, a low R-squared may not be a big problem, particularly if the model can generalize to new data. However, if the goal is to understand how different variables impact the outcome variable, a low R-squared can be a problem.\n",
    ">*   **Model complexity:** A low R-squared can be an indication that your model is too complex, and that it includes too many predictors that are not useful.\n",
    ">\n",
    ">### Key Takeaways\n",
    ">\n",
    ">A small R-squared with good p-values doesn't necessarily mean that your model is bad. It is important to carefully evaluate the model, consider the goal of the analysis, and consider factors such as omitted variables, multicollinearity, and model complexity.\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac923c2",
   "metadata": {},
   "source": [
    "# Post-lecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2533be59",
   "metadata": {},
   "source": [
    "## 7."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3040b923",
   "metadata": {},
   "source": [
    "Each new model is a refined version of the previous model, either through adding, dropping, narrowing, or transforming predictors. By doing so, it can more accurately predict the outcome variables and fit the data better than the previous model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601df436",
   "metadata": {},
   "source": [
    "### NotebookLM Session Summary\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Summary</u></summary>\n",
    "\n",
    ">Certainly! Here’s a summary of our discussion:\n",
    ">\n",
    ">1. **Model Development and Refinement**:\n",
    ">   - We examined a series of regression models predicting `HP` in a Pokémon dataset, moving from simpler to more complex models (`model3_fit` to `model7_linear_form`).\n",
    ">   - Each model builds on the previous one, refining by adding, removing, or narrowing predictors and interaction terms to improve accuracy, reduce overfitting, and enhance interpretability.\n",
    ">   - As each model progressed, it either added variables (e.g., categorical indicators for Pokémon types and generations), dropped insignificant ones, or selectively included interactions to balance predictive power and stability.\n",
    ">\n",
    ">2. **Multicollinearity**:\n",
    ">   - We discussed multicollinearity, which occurs when predictor variables are highly correlated, causing overlapping information and complications in regression analysis.\n",
    ">   - Key effects of multicollinearity include unstable coefficient estimates, reduced interpretability, inflated standard errors, and potential computational issues.\n",
    ">   - Methods to detect multicollinearity include checking the Variance Inflation Factor (VIF) and correlation matrices.\n",
    ">\n",
    ">3. **Strategies to Mitigate Multicollinearity**:\n",
    ">   - Centering and scaling predictors helped reduce multicollinearity, especially for models with many interactions (e.g., in `model7`).\n",
    ">   - Dropping redundant predictors or using regularization methods were also noted as ways to handle multicollinearity effectively.\n",
    ">\n",
    ">In summary, we explored how successive model refinements can improve performance and interpretability in regression analysis, with special attention to managing multicollinearity to stabilize and optimize model outcomes. Let me know if you need any further details or examples!\n",
    "    \n",
    "</details>\n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Log</u></summary>\n",
    "\n",
    "[STA130 - Week 10 HW 8 - Question 7](https://chatgpt.com/share/6736d162-b08c-8001-95da-f6573306a9a2)\n",
    "    \n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
